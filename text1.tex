\documentclass[11pt, oneside]{article}   	% use "amsart" instead of "article" for AMSLaTeX format
\usepackage{geometry}                		% See geometry.pdf to learn the layout options. There are lots.
%\geometry{letterpaper}                   		% ... or a4paper or a5paper or ... 
\geometry{a4paper}                   		% ... or a4paper or a5paper or ... 
%\usepackage[parfill]{parskip}    		% Activate to begin paragraphs with an empty line rather than an indent
\usepackage{graphicx}				% Use pdf, png, jpg, or epsБ•√ with pdflatex; use eps in DVI mode
								% TeX will automatically convert eps --> pdf in pdflatex		
\usepackage[normalem]{ulem}
\usepackage{amssymb}
\usepackage[colorlinks]{hyperref}
\usepackage{booktabs}
\usepackage{amsmath}
\usepackage{multirow}% http://ctan.org/pkg/multirow
\usepackage{tablefootnote}
\usepackage{import}
\usepackage{subcaption}
\usepackage{caption}
\usepackage[format=hang,font=small,labelfont=bf]{caption}

\newcommand{\COMMENT}[1]{{\color{red} #1 }}

\title{A quantitative assessment of the Hadoop framework for analysing massively parallel DNA sequencing data (Hadoop Or Not Hadoop)}
\author{Alexey Siretskiy,  Luca Pireddu, Ola Spjuth \COMMENT{[specify affiliation]}} 
%\date{}							% Activate to display a given date or no date

\graphicspath { {./figures/fig1/} {./figures/fig2/} {./figures/fig9/} {./figures/fig5/}}

\begin{document}
\maketitle

\abstract{New high-throughput technologies such as massively parallel sequencing has transformed the life sciences into a data-intensive field. With increasing data volumes comes the necessity to analyse data in parallel using high-performance computing resources, but doing this effectively can be laborious and challenging. 
Hadoop, emerging in the last decade, is a framework that automatically distributes data and computation and has been shown to scale to thousands of nodes. Herein we report a quantitative comparison of Hadoop to regular high-performance computing resources for aligning short reads and calling variants for five datasets of different sizes up to 250\,gigabases. In order to increase performance of existing software and obtain a better comparison we modified and wrote new analysis scripts. From the observered scaling relations we are able to draw conclusions about the perspectives of the approaches, leading to the conclusion that as data set sizes reach 100\,gigabases, the Hadoop-based pipelines become performance-competitive with a canonical high-performance cluster solution. As data sets in biological sequencing are sure to increase with time, Hadoop and similar frameworks are very interesting technologies that we envision will play a key role in the future of biological data analysis. \COMMENT{Brit spelling assumed since that's most common in this document}
}


\section{Introduction}
\label{sectionI}
%The problem: dealing with BIG data
Since its inception, massively parallel DNA sequencing, also referred to as Next Generation Sequencing~(NGS) technology, has been an extremely bountiful source of data giving insight into the workings of biological machinery~\cite{metzker, marx}. Decreasing sequencing costs facilitates and promotes larger and larger studies with increasingly larger data sizes, and extracting useful information from these voluminous amounts of data is transforming biology into a data-intensive discipline.  As an example of the scale of the demands, consider that a single Illumina high-throughput sequencing run produces approximately 3\,TB of raw data in 10 days~\cite{illumina}.  Indeed, the Swedish UPPMAX\footnote{\url{uppmax.uu.se}} \COMMENT{[unusual to use both footnotes and endnotes]} high-performance computing (HPC) center recently disclosed data showing that just in their sequencing context (most sequencing performed in Sweden) \COMMENT{huh?} storage is being occupied at a rate of~1\,TB/day while the analyses are using over~1 million computing core-hours per month~\cite{lampa}. 

%Current practices: HPC systems
A common step of NGS data analysis consists of mapping short reads to a reference sequence and then finding the genetic variations specific to the sample. \sout{Most of the bioinformatic programs are written for the Linux operating system.} /COMMENT{awkward but maybe it's needed} Some of the most widespread tools\footnote{average number of invocations per month during 2013 at UPPMAX: Samtools~-- 30000, BWA~-- 25000, Bowtie~-- 5000; from internal communications} like BWA~\cite{bwa}, Bowtie~\cite{bowtie} and Samtools~\cite{samtools} are ``regular'' computer programs, not made with distributed computing in mind. Many others do not even have the native ability to use multiple cores \sout{on the same computer}\footnote{Samtools acquired this feature in v.0.1.19}.

%Current practices: How to parallelize

The most common approach to speed up NGS tools is to parallelise within a compute node using shared memory parallelism~(OMP)~\cite{openmp}, but this approach is naturally limited by the number of cores per node, which does not usually exceed~16.  For the tools which do not support the OMP natively, e.g. Samtools, variant calling can be parallelised by creating a separate process  for  each chromosome, or using GNU Parallel~\cite{gnuparallel} Linux utility.
Of great importance is that a multi-core approach does not improve the performance of operations that are limited by disk or network throughputs, motivating \COMMENT{huh?} to split the dataset and use multi-node parallelisation. 

Message Passing Interface~(MPI)~\cite{mpi1} is a common way to implement multi-node parallelisation, but writing efficient MPI-programs for hundreds of cores is a non-trivial task since thread synchronisation (or load balancing) has to be woven into the software code \sout{by a programmer} and there are only a few existing solutions available for processing sequencing data~\cite{pmap, erne, gnumap}.

Another common way to introduce parallelisation to NGS analysis pipelines in Linux systems is to use Bash scripting. This involves using existing utilities and cluster tools to split the data into chunks, process them on the separate nodes, and merge the results afterwards. This kind of solution benefits from both MPI-like and OMP parallelisation and provides good performance, but the development requires substantial expertise in order to be efficient. Since the process is tightly coupled to the local computational cluster and network architectures, it might not be possible to be re-used in other settings.

%Introduce Hadoop, MR, and HDFS
%Properties of Hadoop and HDFS- data localization and programming model etc
The Map-Reduce~(MR) programming paradigm~\cite{hadoop} offers a compelling alternative for running tasks in a {\it massively} parallel way. This paradigm, however, shifts the focus from the best performance to scalability, suited for managing huge datasets of sizes up to several terabytes~\cite{lin2010}.
The most prevalent open source implementation of Map-Reduce is Hadoop~\cite{hadoop,Hadoop:Guide}. \COMMENT{Fix bib error (can't have both editor and author for this ref type)} 
The Hadoop~MR framework provides automatic distribution of computations over many nodes as well as automatic failure recovery (failure of individual jobs or computing nodes by storing multiple copies on different nodes), and automated collection of results~\cite{Hadoop:Guide}. Hadoop Distributed File System~(HDFS) is a complementary component that stores data by automatically distributing it over the entire cluster, writing data blocks onto the local disk of each node and therefore effectively moving the computation to the data and reducing network traffic. HDFS provides a storage system whose bandwidth and size scales with the number of nodes in the cluster~\cite{Sammer:2012}, which is very different from the properties of the usual HPC cluster network architecture. 

%Manuscript focus: When is Hadoop currently an appealing alternative
%Introduce what we did in the manuscript.
In this manuscript we focus on the question {\it if} and {\it when} \COMMENT{that's two questions} Hadoop is an appealing alternative to the program tools generally found in~HPC centers for DNA-seq analysis. Since Hadoop is written in Java, which is slower than the standard~HPC programming languages like C or Fortran, we seek to estimate an average data size when it starts to be worthwhile to use Hadoop from a performance perspective. We use five datasets with short reads of different sizes, align them against the appropriate reference genome, and call the variances.
For the existing Hadoop software we propose modifications, in order to benefit fully from massively parallel nature of MR computations. 
For the classical HPC DNA-seq analysis programs we developed a set of Bash scripts utilizing multiple nodes for short read alignment and exploiting the network fully, thereby speeding up calculations.
The execution times for each dataset were collected and scaling relations were analysed to answer the question\COMMENT{s} in focus.

The manuscript is structured as follows: \COMMENT{[odd to do this in a normal publication, but OK]} In Section~II we briefly introduce the datasets, computational facilities, analysis pipelines design,  and the software used. Then, Section~III presents experimental results; first verifying that both HPC and Hadoop approaches extract the same mutation and then investigating the scaling relations in terms of data size and computing resources. The results are discussed in Section~IV, and conclusions in Section~V. Supplementary materials are provided in the corresponding section.



\section{Methods}
\label{sectionII}

\subsection{Datasets}
We used publicly available DNA-seq datasets~(I--III), a synthetic dataset~(IV) of {\it A.thaliana}, the well-known model plant, and dataset (V) of two {\it H.sapiens} individuals (Table~\ref{table:datasets}). Data for datasets I-III and V were obtained using Illumina/HiSeq sequencing platforms. Further information about the datasets is provided in the Supplementary material section.


\begin{table}[htdp]
\small
\footnotesize
\caption{Datasets used}
\begin{center}
\begin{tabular}{|l|l|l|}
dataset &	organism &	size in Gbases\\
\hline
 I		&	{\it A.thaliana}	&	1.4	\\
 II	&	{\it A.thaliana}	&	7.0\\
  III	&	{\it A.thaliana}	&	30.0	\\
 IV	&{\it A.thaliana}, the artificial dataset created using Samtools package	&	100.0	\\
 V	&	{\it H.sapiens}, two individuals (GM12750 and GM12004), sample SRR499924		&	250.0\\

\end{tabular}
\end{center}
\label{table:datasets}
\normalsize
\end{table}%


\subsection{Analysis pipelines}
We constructed two pipelines for identifying single-nucleotide polymorphisms (SNPs) from short read data, one based on Hadoop and the other on regular HPC with a batch processing system (hereafter referred to as HPC). Our experiments then consisted of running the pipelines for the selected input dataset and measuring the wall-clock run time for each pipeline stage. All experiments were repeated several times and the results averaged \COMMENT{considering the variations in table 4, why not use medians} to obtain a data point for the particular combination of data size and computational platform. Acknowledging that there are different approaches and software for conducting bioinformatic analysis for HPC (e.g., GATK~\cite{gatk}), we decided to create the  analysis pipeline as simple as possible to be able to pass \COMMENT{huh?} the same stages on Hadoop and HPC:


\begin{enumerate}
\item HPC approach
\subitem Short read alignment: Bowtie~ver.~0.12.8
\subitem SNP calling: Samtool~ver.~0.1.19
\item Hadoop approach: Crossbow~\cite{crossbow}
\subitem Short read alignment: Bowtie~ver.~0.12.8
\subitem SNP calling: SOAPsnp~1.02~\cite{soapsnp}
\end{enumerate}


In the HPC pipeline, reads were aligned with Bowtie, followed by sorting the mapped reads and~SNP calling with Samtools. The Bowtie aligner natively implements~OMP, meaning that with~8 cores on the same computer the result can be theoretically obtained 8 times faster than on a single core. Likewise, Samtools (as of version~0.1.19) also offers shared memory parallelism for several of its functions. Where available these features were used to improve the analysis speed. The exact workflow used is available in the code repository created for this work~\cite{code_repo}. \COMMENT{missing ref}

The equivalent Hadoop-based pipeline was implemented with Crossbow. The input data and the indexed genome reference were copied to Hadoop's storage system~(HDFS) before starting the experiments. Crossbow implements a short pipeline that pre-processes the input data, transforming it into a format suitable for the alignment stage, and then continues to use Bowtie for alignment and SoapSNP to call SNPs.  Unfortunately, Crossbow's preprocessor is not written in MR manner, and thus cannot be run in a massively parallel way. Due to this limitation, this basic step threatened to be the most time-consuming procedure in our test pipeline and bias our experiments. To overcome this bottleneck we substituted Crossbow's preprocessor with our own MR implementation.



\subsection{Computational resources}
To run the HPC analysis pipeline we used a computational cluster at UPPMAX on nodes equipped with 8 dual-core CPUs. Data and reference genomes were stored on a parallel, shared-storage system\cite{gulo}. The Hadoop test platform was deployed on a private cloud at UPPMAX using the OpenNebula~\cite{opennebula} cloud manager. The cluster was set up with Cloudera Hadoop distribution version~2.0.0-mr1-cdh4.5.0~\cite{cloudera}. For details on computational resources, see Supplementary material.





\section{Results}
\label{sectionIII}

\subsection{Modified preprocessing stage}
A native preprocessing stage in Crossbow\footnote{the same holds for Myrna, a cloud-based solution for differential expression analysis} provides great flexibility in delivering the data to the Hadoop cluster. Data can be downloaded from Amazon S3, FTP and HTTP servers over the Internet\cite{crossbow}. The only way to introduce parallelisations in the native preprocessing stage is to split the read files into smaller chunks and to generate a manifest file listing all these chunks. This, however, is lacking the massively parallel way of treating the data.

Being pragmatic we assume that sequencing platforms are usually affiliated with computation facilities which provide both storage and computers, therefore we considered that the data already been downloaded to storage Hadoop could access.
We rewrote the preprocessing stage scripts in a MR-manner, benefiting from its massively parallel nature.
The requirement is that the FASTQ data are BZIP2 archived and accessible by Hadoop. For our case the storage with the data was mounted with SSHFS to one of the Hadoop nodes, from where the BZIP2'ed data were ingested \COMMENT{special word?} to the HDFS.
BZIP2 provides very good compression and is splittable, meaning that the archive can be expanded in a parallel manner\footnote{other options like splittable LZO are also possible}. Also, BZIP2 format is natively supported by Hadoop by enabling the respective codec, i.e., \sout{for the developer} there is no difference in dealing with the FASTQ data or with its BZIP2 archive.

The Hadoop streaming library offers a possibility to write Mapper and Reducer for Hadoop jobs in any programming language. Our Python scripts efficiently process short reads, produced by Illumina sequencing platform of different versions, as well as the FASTQ files converted from SRA format (NCBI Sequence Read Archive).\COMMENT{ref?} 
The problem of lacking the unique FASTQ header standard was solved by the rewriting the header based on a SHA-1 hash function, and the reads mates were labeled with ``.1" and ``.2" suffixes. 
To ensure that both the forward and reverse reads of the same pair would end up on the same Reducer, secondary Mapper key-sorting mechanisms were involved.

In order to compare the native preprocessor and the proposed one, the data was put on the HDFS beforehand, to eliminate possible network delays, e.g., while downloading data from some Amazon cloud.
Table~\ref{table:preprocess} illustrates the benefits of our approach.
\begin{table}[htdp]
\small
\caption{Timings (in minutes) with the  Crossbow native read preprocessor and with derived approach for  datasets II, III, IV  on \sout{$p=56$} 56 cores Hadoop cluster, using the Crossbow, and with the BASH script on a single HPC node \sout{, $p=16$} with 16 cores. \COMMENT{A bit odd to use the vertical lines before/after table, but OK.}}
\begin{center}
\begin{tabular}{|l|c|c|c|c|}
Dataset			&		II (7 Gbases )	& III (30 Gbases)	& IV (100 Gbases)\\
\hline
Crossbow native			&		$60.6\pm0.7$	& $299.0\pm2.3$	&	$673\pm1.0$	\\
Hadoop, this work			&		$7.0\pm0.0$	&	$20.7\pm0.4$&		$52.4\pm0.1$\\
BASH, this work			& 		$7.4\pm0.0$	&	$31.1\pm0.1$	&	$114.5\pm0.3$	\\
\end{tabular}
\end{center}
\label{table:preprocess}
\normalsize
\end{table}%
To complete the comparison the same preprocessing was performed with the BASH scripting against the data located on the HPC storage. The script\footnote{\texttt {pbzip2 -dc  \${file1} | paste - - - - -d'\textbackslash t' | cut -f1,2,4 | paste - -d' ' <(pbzip2 -dc  \${file2} | paste - - - - -d'\textbackslash t' | cut -f2,4) | pbzip2 -cz > \${fileOut}}} ran on a single HPC node, utilizes multiple cores, with the limiting factor being the HDD IO performance, which effectively leaves with 5-7 cores of 16. \COMMENT{awkward}



\subsection{Accuracy of pipelines}
Since our HPC and Hadoop approaches use different~SNP callers (Samtools and SOAPsnp, correspondingly) we should not expect them to deliver perfectly matching SNP lists, but still we expect them to capture and correctly identify the mutations. We tested the correctness just for the smallest dataset (I). The mutation $C\rightarrow T$ on chromosome 4 at position $16702262$\cite{schneeberger} was successfully localized by both applications.



\subsection{Scalability of HPC and Hadoop approaches}
To demonstrate the scalability of the HPC approach we collected running times for dataset~I as a function of the number of cores used (Table~\ref{table:3}). \COMMENT{Consider replacing with a graph.}
\sout{As one can see} the aligning process with the Bowtie scales fairly well, but the SNP calling part is a bottleneck, scaling worse than the Bowtie, and consuming a progressively larger portion of the total calculation time.
%\footnote{The amount of available memory, specified with the \texttt{-m} option, is of big importance for the Samtools performance. During its sorting phase the lack of RAM results in spilling to the HDD. For example for an half of  Dataset III, SNP calling takes approximately 40 minutes with 8 GB RAM, and 15 minutes with 48 GB RAM, when  the whole BAM file fits into RAM. For our tasks we used the default Samtool's settings.}.  
For the given datasets (I--V) the timings for alignment against the corresponding genomes and SNP calling for the HPC and Hadoop approaches were collected (Table~\ref{table:4}).

One of the attractive sides of using Hadoop~MR is its almost linear scalability, i.e., calculation time linearly depends on the size of the dataset\cite{crossbow,seal}, regardless the scaling nature of the underlying program (Samtools, Bowtie, etc.).
Figure~\ref{fig:fig1} \COMMENT{this figure should be closer, table and figure layout odd in general}, based on Table~\ref{table:4}, shows the calculation time as a function of the dataset size for $p=56$ \COMMENT{is this 'p=' notation necessary} cores Hadoop cluster. Dataset V was excluded since it is for {\it H.sapiens}, which has more than 20 times larger genome than~{\it A.thaliana}. \COMMENT{so what}
To stress the linear nature of scaling, both sets of points were fit \sout{to linear polynomial} using the least squares method.


\begin{table}[htb]
\small
\caption{Calculation time for Dataset I executed on different number of cores on a node of HPC cluster. \COMMENT{awkward title, consider replacing with graph (and new title)}}
\begin{center}
\begin{tabular}{|c|c|c|c|ccc|}
$N$ cores	&\multicolumn{3}{c|}{timing, minutes}&\multicolumn{3}{c|}{speed-ups} \\
\hline
	& mapping 	&	SNP calling\tablefootnote{We used special tricks to parallelize the Samtools analysis by chromosome, as exemplified here \url{http://www.biostars.org/p/48781}.}	&	total  &total & map& SNP call\\
\hline
1	&	22.75	$\pm$1.24&	26.1$\pm$0.7	&	48.9	&	 1.00	&	1.00	&	1.00\\
2	&	11.00$\pm$0.05	&	16.9$\pm$1.3	&	27.9	&	 0.88	&	1.03	&	0.77\\
4	&	5.91$\pm$0.04	&	11.7$\pm$0.5	&	17.6	&	 0.69	&	0.96	&	0.56\\
6	&	4.24$\pm$0.04	&	8.6$\pm$0.8	&	12.8	&	 0.64	&	0.89	&	0.51\\
8	&	3.44$\pm$0.03	&	8.6$\pm$0.9	&	12.1	&	 0.51	&	0.83	&	0.38\\
10	&	2.91$\pm$0.01	&	7.7$\pm$0.5	&	10.6	&	 0.46	&	0.78	&	0.34\\
12	&	2.64$\pm$0.04	&	7.5$\pm$0.5	&	10.1	&	 0.40	&	0.72	&	0.29\\
14	&	2.57$\pm$0.03	&	7.5$\pm$0.6	&	10.1	&	 0.35	&	0.63	&	0.25\\
16	&	2.88$\pm$0.06	&	7.4$\pm$0.4	&	10.2	&	 0.30	&	0.49	&	0.22\\
\end{tabular}
\end{center}
\label{table:3}
\normalsize
\end{table}

\begin{table}[htdp]
\small

\caption{Timings  (in minutes) for HPC and  Hadoop deployments for different dataset sizes.    Dataset is shown in brackets by roman numerals, ``f.r.'' stands for ``forward reads''. Big deviation \COMMENT{what aspect of deviation, not fixable?} for the HPC is due to Samtools BAM sorting, which is IO and memory intensive, thus strongly depend on the queuing at the HPC cluster. \COMMENT{No p= used here.} }
\begin{center}
\begin{tabular}{|l|c|c|c|c|c|c|c|}

data, Gbases		&	1.4	&	3.5		&	7.0		&	15.0		&	30.0		&	100.0	&	250 	\\
				&	(I)	&	(II, f.r.)	&	(II)		&	(III, f.r.)	&	(III)		&	(IV)		&	(V)\\
\hline
Hadoop, 56 cores&--&	--	&39		&62	&108	&250&1125\\
Hadoop, 56 cores		&	$18\pm0	$	&	$18\pm0	$	&	$29\pm0$	&	$47\pm0	$	&	$89\pm0$	&	$238\pm1$		&	$1164\pm14$\\
HPC1, 8 cores&	41&	96	&157	&307	&596	&1490&--\\
HPC, 16 cores	&	$17\pm1$	&	$22\pm6$	&	$43\pm3$	&	$81\pm9$	&	$172\pm15$		&	$467\pm60$	& $>48$ hours\\

\end{tabular}
\end{center}
\label{table:4}
\normalsize
\end{table}%




\begin{figure}
	\input{./figures/fig1/fig1.tex}
	\caption{Calculation time depending on the size of selected datasets for~$p=56$ \COMMENT{p= returns} cores Hadoop cluster on the private Cloud. \sout{The least squares fit reveals linear scaling.} Datasets are marked with roman numerals. \COMMENT{Rather large graph for such a simple result.} }
	\label{fig:fig1}
\end{figure}


\subsection{Comparing Hadoop and HPC runtime efficiency for different dataset sizes}
\COMMENT{delete 'for different dataset sizes'} Hadoop was designed to digest huge datasets\cite{hadoop,lin2010}. One can propose then that the larger the dataset is, the more suitable Hadoop becomes compared to the HPC approach. In order to compare the ``suitability'' for different types of calculation platforms (Hadoop and HPC) each being run on a different number of cores, we constructed the following function:
$$F=T_{p}\times p,$$ 
where~$T_{p}$ is the calculation time on~$p$ cores. \COMMENT{This trivial/obvious formula doesn't deserve such special treatment.}
Using the data from Table~\ref{table:4}, we plot the ratio $F_{Hadoop}/F_{HPC}$ \sout{, keeping in mind that the closer the ratio is to the unity, the closer Hadoop's efficiency is to that of the HPC approach}.

\begin{figure}
	\input{./figures/fig2/fig2.tex}
	\caption{The ratio of the~$F_{Hadoop}/F_{HPC}$ as a function of a reciprocal dataset size in Gigabases. Calculations were carried out for~$p=56$ and $p=16$ cores for Hadoop and HPC correspondingly.
The points are fit to a quadratic least-squares curve, which makes it possible to predict {\it infinite} dataset size. Datasets are marked with roman numerals.}
	\label{fig:fig2}
\end{figure}

The curve  in~Figure~\ref{fig:fig2}, based on Table~\ref{table:4}\sout{, is plotted for~$p=56$ cores Hadoop cluster and an HPC node ($p=16$), and} displays the ratio~$F_{Hadoop}/F_{HPC}$ as a function of the reciprocal dataset size. Extrapolation to zero on the~$X$-axis estimates the ratio for the hypothetical infinite dataset. As one can see, the Hadoop approach becomes more and more effective compared to the HPC scenario as the dataset size increases. 
The parabolic extrapolation for the ratio is $1.70\pm0.01$, meaning that Hadoop running even in the virtualized environment of a private cloud assembled on moderate hardware, is competitive with the HPC approach run on ``bare metal'' of modern hardware for datasets greater than $100$\,Gbases (dataset~IV), which is typical for human genome sequencing with a sequencing depth of about~$30x$.

The simplest curve to fit the points nicely is a parabolic function, importantly not a linear one. Considering a {\it linear} scaling for the Hadoop approach for those datasets, Figure~\ref{fig:fig1}, one can conclude that HPC approach scales worse than linearly\footnote{For the linear scaling the curve of $F=T_{p}\times p$ does not depend on the dataset size, i.e. being a constant; thus the ratio of two constants would result a constant, but we observe a dependecy.}. \COMMENT{Why jump back and forth in the discussion of figures 1 and 2?} We suspect the main reason for such a behavior is the fact that for large datasets (starting from dataset~III in our case) the SNP calling routine with the Samtools becomes more memory and time demanding than the actual mapping with Bowtie.
If lacking  RAM, the Samtools swaps to disk, creating multiple (up to hundreds) temporary files while sorting the BAM file, keeping the network load for NFS storage and time-expensive IO on a high rate.
Table~\ref{table:5} shows the role of the amount of allocated RAM on the Samtools sort timing, as a most time expensive \COMMENT{huh?}, for the dataset~IV, starting from 8GB up to 80GB, performing on a single HPC node. The large uncertainty for small amounts of RAM we believe is due to other users' load on the cluster network. As a result, a large amount of RAM per node is needed in order to use Samtools effectively, while Hadoop accomplishes SNP calling on much more economic hardware in linear time with just 12GB RAM per VM node.

 
\begin{table}[htdp]
\small

\caption{Timings (in minutes) for the sort routine of the Samtools package for the dataset IV (58Gb BAM file). The measurements are done on a single HPC node with $p=16$ cores. Averaging is done over 5 independent simulations. The amount of RAM is given for all 16 cores. }
\begin{center}
\begin{tabular}{|l|c|c|c|c|c|c|c|}
RAM, GB		&	8			&		16			&			32		& 		64				\\
timing, minutes	&	$208\pm50$	&	$172\pm13$		&	$136\pm12$		& 	$87\pm11$			\\
\end{tabular}
\end{center}
\label{table:5}
\normalsize
\end{table}%



 \subsection{Comparing the network communication efficiency for the Hadoop and HPC approaches }
The network communication model for Hadoop has a major difference from the usual HPC cluster network architecture ($n$-tier tree) with NAS or NFS attached storages. The effective bandwidth of the Hadoop network increases with the cluster size~\cite{Sammer:2012}, opposite to that of the HPC network where cluster growth results in network saturation and performance depletion.
We provide a comparison of HPC and Hadoop network communication costs depending on the number of nodes involved for a fixed dataset size (58~Gb, dataset~IV).
 
Due to the trivially parallelisability of the alignment process~-- the read-pairs are independent of each other, and can be aligned independently~-- one could try to involve more computational resources, e.g., split the initial data into chunks to process them independently.
Reducing the size of each data chunk reduces the aligner  job,~$T_{mapping}$, but at the same time, the more chunks almost simultaneously have to be sent over the network, potentially causing traffic jams, and therefore increasing the communication costs,~$T_{comm}$.

There are several program packages for short read alignment with MPI support~\cite{pmap, gnumap}. Authors report almost linear scaling up to 32 nodes for pair-ended reads\footnote{\url{http://bmi.osu.edu/hpc/slides/Bozdag10-HiCOMB.pdf}, \url{http://dna.cs.byu.edu/gnumap/HICOMB_Presentation.pdf}}. \COMMENT{why footnote rather than endnote} However, e.g., the Pmap package functions poorly \sout{on UPPMAX cluster} for datasets larger than 20 Gbases \sout{, raising memory exceptions}. \COMMENT{Needs an intro like To circumvent this limitation} We implemented a highly optimized Bash script making use of standard Unix utilities to use the HPC cluster network as efficiently as possible~\cite{code_repo}, \COMMENT{missing reference, written as 'repo' but assumed to be same as 'code\_repo' above} and compared the network performance with the standard Hadoop HDFS approach. We separated the mapping time, $T_{mapping}$, and the communication time, $T_{comm}$, and plotted $T_{mapping}/T_{comm}$ as a function of the reciprocal number of nodes~$1/N$ (Figure~\ref{fig:fig3}). 
This measure is applicable to both HPC and Hadoop, however, $T_{comm}$ has a different explanation. For Hadoop, the short reads in FASTQ format have to be preprocessed (involving node communication~$T_{comm}$) to be able to run in MR-fashion, while the data locality will be automatically achieved during the data ingestion into the HDFS. 
We rewrote the code for the preprocessing stage for Crossbow to make it suitable for MR-style parallelisation.
For the HPC approach, $T_{comm}$ involves the chunks that are sent from the sequence delivery location to the local node scratch disks where the actual mapping happens, and the sending of the aligned SAM\footnote{\url{http://samtools.sourceforge.net/SAMv1.pdf}} files back to the delivery location over the network.

Figure~\ref{fig:fig3} shows the~$T_{mapping}/T_{comm}$ ratio as a function of the reciprocal number of nodes~$1/N$ for Hadoop and HPC approaches. \COMMENT{why describe fig 3 again} The HPC approach is presented in two versions that are based on a bit different strategies of the resource allocation.  

\textit{Hadoop results (filled circles in Figure~\ref{fig:fig3})}: One can see that the ratio~$T_{mapping}/T_{comm}$ reveals very weak dependency in a wide range of number of nodes~$N$: from 4 up to 40. It is known that Bowtie provides almost linear scaling between mapping time and dataset chunk size~$D$:~$T_{mapping}\propto  D\propto 1/N$, see~\cite{bowtie}, and Figure~\ref{fig:fig1}. Since the ratio~$T_{mapping}/T_{comm}$ is approximately constant, one can conclude that $T_{comm}\propto 1/N$, meaning that the more nodes involved, the faster the communication is in the preprocessing stage.

\textit{HPC results}: The strategy named HPC SLURM\footnote{Simple Linux Utility for Resource Management}  (open circles in Figure~\ref{fig:fig3}) is as follows: the data from the delivery location is split into chunks in parallel, which are simultaneously pushed to the local scratches of the nodes allocated by SLURM. One can see two distinct linear stretches. One stretch is for the range from 4 to about 12 nodes, and the another is from 12 up to 60. The former (horizontal stretch) is explained as for Hadoop~-- the more nodes involved, the faster the chunks are being distributed. The latter stretch with the positive slope could be explained as follows: In the region of about 12 nodes the network becomes saturated\footnote{The used storage at UPPMAX is a set of RAIDs (Redundant Array of Inexpensive Disks) with data striping, providing up to $80$Gbit/sec of outcoming traffic \COMMENT{'outgoing' or 'incoming' or just 'traffic'}.} and unable to pass more data in a time unit, while the mapping time is still proportional to the chunk size:~$T_{comm}\approx\mbox{const}, T_{mapping}\propto D\propto 1/N \rightarrow T_{mapping}/T_{comm}\propto 1/N$, i.e., a linear dependency, which one can observe in the plot. 
The transition area between the two modes~-- saturated and unsaturated~-- has the next origin \COMMENT{huh?}: each hard disk drive on the local node can write the data at a speed of about \sout{100MB/sec\,$\approx$\,} 1Gbit/sec. Ten nodes will consume the data with the rate of~10Gbit/sec, which is the limiting speed for the standard 10Gbit Ethernet cable connecting the cluster's rack with the switch. The nodes are being allocated on the same rack, which is the default SLURM behaviour.

Scalability can be improved by overriding the default behaviour of SLURM and allocating the nodes not from the same rack, but randomly from all available racks (``HPC random'', open squares in Figure~\ref{fig:fig3}). Allocating the nodes on random racks allows one to engage more nodes without network saturation. For our cluster we could go up to 30-35 nodes with perfect linear scaling. For the most resources used (58 nodes) the deviation from a linear speedup is~$\approx 7\%$ i.e. $5.50$ minutes against the ideal $5.14$, see Table~\ref{table:4}  for the data. The threshold number of nodes in this strategy~($\approx35$) is because of the uplink cable with the throughput of~50Gbit/sec being saturated. 
The proposed HPC strategies aimed at getting the maximum performance from the storage resources show that while even properly adjusted and tuned, the HPC approaches suffer from the network saturation at higher number of nodes.


\begin{figure}
	\small
	\input{./figures/fig9/fig3.tex}
	\normalsize
	\caption{Ratios of the mapping time~$T_{mapping}$ to the communication costs~$T_{comm}$ for HPC and Hadoop clusters for Dataset IV as a function of reciprocal cluster size $1/N$. Two HPC scenarios are shown as ``HPC SLURM'' and ``HPC random'', which correspond to standard SLURM behaviour and a modified one where nodes are being allocated from random racks.
%One  can see the differences in the Hadoop and HPC communication patterns: for Hadoop the ratio is almost constant, while for HPC cluster there are two defined regions.
%The first one resembles to that of Hadoop, and corresponds to the case then the communication in the cluster is not a bottleneck. 
%The second one shows the regime corresponding to the case of saturation of the network, resulting in traffic jams: data distibution time~$T_{comm}$ stops to scale 
%linearly to the reciprocal cluster size, and becomes more of less a constant, while the mapping time continues to satisfy~$T_{mapping}\propto 1/N$.
Linear fit  done with the least-squares method. \COMMENT{Figure legend too complex. Y-axis label inconsistant with previous figure.}}
	\label{fig:fig3}
\end{figure}



\begin{table}[htdp]
\caption{Timings  for mapping and the ratio~$T_{mapping}/T_{comm}$ for HPC and Hadoop clusters for Dataset IV.
For the ``HPC random'' approach, data chunks have to be copied to the local scratch disks first and the alignments (SAM files) copied back while Hadoop keeps all the data inside HDFS and hence does not need data staging. Hadoop however needs to preprocess reads before the actual alignment stage in order to be able to operate in MR manner resulting in what we term ``communication costs''. Note that each HPC node has 16 cores, while each Hadoop node has 7 (one core is dedicated to run the virtual machine).}
\begin{center}
\begin{tabular}{|c|c|c|c|c|c|}
 \multicolumn{3}{|c|}{Hadoop} & \multicolumn{3}{c|}{ HPC random} \\
 \hline		


Number of nodes	&Mapping time,	&$\frac{T_{mapping}}{T_{comm}}$	&Number of nodes	&Mapping time,	&$\frac{T_{mapping}}{T_{comm}}$\\
(cores)					&minutes		&							&(cores)			&minutes&\\
\hline
4(28)	&293.5	&2.33	&4(64)	&74.4	&3.89\\
6(42)	&189.8	&2.19	&10(160)	&32.4	&3.76\\
8(56)	&136.0	&2.23	&14(224)	&22.7	&3.77\\
16(112)	&70.3	&1.96	&18(288)	&17.9	&3.78\\
32(224)	&39.3	&2.20	&22(352)	&14.5	&3.79\\
40(280)	&32.5	&2.15	&26(416)	&12.3	&3.77\\
			&&&30(480)	&10.7	&3.73\\
			&&&34(544)	&9.5	&3.45\\
			&&&38(608)	&8.5	&3.16\\
			&&&42(672)	&7.6	&2.96\\
			&&&46(736)	&7.0	&2.55\\
			&&&50(800)	&6.4	&2.65\\
			&&&54(864)	&5.9	&2.34\\
			&&&58(928)	&5.5	&2.12\\

\end{tabular}
\end{center}
\label{table:6}
\end{table}%
 At the same time, the HDFS keeps data locality \COMMENT{'keeps data locally' or 'maintains data locality'}, aiming to reduce the amount of communications, resulting less data move and, therefore, better scalability.
Our Hadoop-in-the-Cloud cluster has no more~($\approx 40$) \COMMENT{how does 40 approximate zero?} free nodes to continue to investigate the scaling as in plot at Figure~\ref{fig:fig3}, but we do not expect any significant deviations, since the observed behaviour is a generic for Hadoop with HDFS. \COMMENT{need ref?}


\subsection{Usability aspects}
\label{subsectionIV_2}

A popular way to construct and execute bioinformatic pipelines on HPC resources is via the Galaxy\cite{galaxy} platform, which provides a Web-based graphical user interface~(GUI) to bioinformatic programs, simplifying the experience for the end user. 
One of the alternatives for Hadoop is Cloudgene\cite{cloudgene}, which is a light-weight and flexible Web-based solution for both public and private clouds. We implemented our Hadoop-pipeline in Cloudgene and extended the platform with functions to import data from the central file system at UPPMAX into HDFS on the private cloud (Figure~\ref{fig:fig4}(a)).
For our particular task in DNA sequencing, Cloudgene provides the intuitive interface making one easy to follow. Most of the data managing work is done automatically and the results can be downloaded to the client machine. The modular structure allows modification of the source code to adapt to the existing computing center’s architecture, Figure~\ref{fig:fig4}(b). For example, UPPMAX users can import their data from the sequencing platform directly to the Hadoop cluster by pressing a button and entering the credentials, being at the same time sure that their sensitive data will be held locally, reducing the amount of unnecessary risks.

\begin{figure}
 	\begin{subfigure}[b]{0.4\textwidth}
        	\includegraphics[width=\textwidth]{a.png}
		\subcaption{}
	\end{subfigure}
	\hspace{0.5cm}
	\begin{subfigure}[b]{0.6\textwidth}
	        	\includegraphics[width=\textwidth]{c.png}
		\subcaption{}
	\end{subfigure}
%	\begin{subfigure}[b]{0.6\textwidth}
%		\includegraphics[width=\textwidth]{b.png}
%		\subcaption{Cloudgene: specifying job parameters}
%	\end{subfigure}
	\caption{An example of a job setup with the graphical Hadoop front-end Cloudgene, providing a smooth user experience even for novice users. \textbf{a)} Pipeline selection - in our case containing the Crossbow pipeline. \textbf{b)} The UPPMAX-adapted functionality to browsing and import data from the user's home folder in the shared file system.}
	\label{fig:fig4}
\end{figure}

\section{Conclusions}
\label{sectionV}

In this report we have described two approaches for high-performance analysis of DNA sequencing data; one based on regular HPC using a batch system and one based on Hadoop. We show that Hadoop installed on a private cloud is an appealing solution both on terms of runtime performance and also in terms of usability. A key result is the data size where Hadoop is favorable to regular HPC batch systems, and we developed highly optimized pipelines for both scenarios. We found that dataset sizes larger than 100\,Gbases is where Hadoop excels over HPC, and also that Hadoop shows almost linear scalability (Figure~\ref{fig:fig1}). Extrapolation to infinite dataset size (Figure~\ref{fig:fig2}) reveals however that HPC provides the results faster, given the same amount of resources as Hadoop on a private cloud.

To increase the performance of the existing Hadoop software, the modification to the preprocessing stage of the Crossbow/Myrna  were suggested.  The calculation time reduction can be observed in the Table~\ref{table:preprocess}. \COMMENT{So, is Hadoop better after the modifications for infinite datasets?}
%A state-of-art Bash script was created to engage multiple nodes (up to 928 cores) in the HPC approach to align the Illumina-produced short reads with almost linear speed-ups.

%We concentrated ourselves on the Hadoop  in the private cloud installation, Hadoop-in-the-Cloud, orchestrated by the OpenNebula. 
%Picking the appropriate program settings we found out that DNA-seq dataset size larger than 100\,Gbases is suitable for Hadoop, with competitive  execution time compare to  that of HPC.
%The scaling graphs (Figure~\ref{fig:fig1}) confirms the known fact for almost linear scalability of Hadoop.
%The extrapolation to the infinite dataset size (Figure~\ref{fig:fig2}) revealed, that HPC, however, provides the results faster, given the same amount of resources, than the Hadoop-in-the-Cloud. 

Exploiting the embarrassingly parallel nature of the short reads mapping we used a highly optimized Bash script to compare the scaling relations between the ratio of the mapping time to the communication time as a function of the reciprocal number of nodes (Figure~\ref{fig:fig3}). 
Our results show that the calculations on Hadoop with HDFS scales better than the network attached parallel storage commonly used in the HPC centers.
In addition we show how performance of the HPC approach can be improved by redefining the queueing system's default behaviour. \COMMENT{Should maybe mention somewhere that this sort of cheating.} Finally, we demonstrate that an existing and extensible publically available web-based GUI (Cloudgene), provides an easy way to execute bioinformatics analysis on Hadoop for those who are less experienced with Linux scripting.



\section{Acknowledgements}
The computations were performed on resources provided by SNIC through Uppsala Multidisciplinary Center for Advanced Computational Science (SNIC-UPPMAX) under project p2013023. This work was supported by the Swedish strategic research programme eSSENCE~\url{http://essenceofescience.se/home/}.
We thank system experts Pontus Freyhult and Peter Ankerst{\aa}l at UPPMAX for valuable discussions on effective storage and network usage. We also thank Jonas Hagberg (BILS, Stockholm, Sweden), for implementing the Cloudgene extensions to import data from UPPMAX filesystem.



\section{Supplementary material}

\subsection{Datasets}

The datasets used in the paper are publicly available at:

I: \url{http://1001genomes.org/data/software/shoremap/shoremap\_2.0\\/data/reads/Schneeberger.2009/Schneeberger.2009.single\_end.gz}

II: \url{http://1001genomes.org/data/software/shoremap/shoremap\_2.0/data/reads/Galvao.2012/Galvao.2012.reads1.fq.gz, http://1001genomes.org/data/software/shoremap/shoremap\_2.0/data/reads/Galvao.2012/Galvao.2012.reads2.fq.gz}	

III: \url{ftp://ftp-trace.ncbi.nlm.nih.gov/sra/sra-instant/reads/ByRun/sra/SRR/SRR611/SRR611084//SRR611084.sra, ftp://ftp-trace.ncbi.nlm.nih.gov/sra/sra-instant/reads/ByRun/sra/SRR/SRR611/SRR611085//SRR611085.sra}

IV: artificial pair-ended dataset for {\it A.thaliana} created with the {\tt wgsim} program from the Samtools package.

V: \url{http://www.ncbi.nlm.nih.gov/sra/SRX148888}


\subsection{Reference genomes}
\begin{itemize}
\item TAIR10 for datasets II-IV \url{ftp://ftp.arabidopsis.org/home/tair/Sequences/whole\_chromosomes/*.fas}
\item TAIR8 for dataset I \url{ftp://ftp.arabidopsis.org/home/tair/Genes/TAIR8\_genome\_release/}
\item H.sapiens, NCBI v37 \url{ftp://ftp.ccb.jhu.edu/pub/data/bowtie\_indexes/h\_sapiens\_37\_asm.ebwt.zip}
\end{itemize}

\subsection{Description of computational facilities}

\begin{enumerate}
%\item  HPC1:
%The HPC analysis pipeline was run on a node from the Kalkyl~\cite{kalkyl} cluster, equipped with two quad-core processors Intel Xeon~5520 (clock frequency of 2.26\,GHz; 1\,MB L2 cache, 8\,MB L3 cache), 24\,GB of RAM and an Infiniband node-to-node network connection, and 10Gbit/s uplink. The data and reference genomes were read and written to a parallel shared storage system. 

\item 
HPC:
Multinode short read mapping was performed on the Milou cluster\cite{milouCluster}, equipped with dual 8--core Intel Xeon E5-2660, (2.2 GHz, 2\,MB L2 cache, 20\,MB L3 cache), 128\,GB of RAM, Infiniband node-to-node network connection, and 10Gbit/s uplink.

\item Storage: 
Gulo\cite{gulo} is a custom built Lustre 2.4 system using 8 HP nodes with MDS600 storage boxes and an additional node for metadata handling. In total, it provides roughly 1 PB of storage and is accessed with Lustre's own protocol. It supports data striping over multiple nodes and disk targets and can give a theoretical single file read performance of up to 80 Gbit per second.

\item Our Hadoop test platform was deployed on a private cloud at UPPMAX using the OpenNebula~\cite{opennebula} cloud management system. Each node in this deployment was equipped with dual 4-core Intel Xeon 5420 (2.50\,GHz; 12~MB L2 cache), 16~GB RAM, one 1~TB SATA disk and Gigabit Ethernet. The cluster was set up with Cloudera Hadoop Distribution version~2.0.0-cdh4.4.0~\cite{cloudera}.
Note that the physical hardware provided less RAM than desired. Each VM node has 7 cores, each of each can use less than 2 GB of RAM, which is little for Hadoop. We would expect much better performance with twice as much memory, as recommended.

\COMMENT{Journal abbreviations in references are inconsistant. The journal name for ref 2 is wrong. Capitalization of refs 4, 14, 16, 18, 19, 25, 27, etc are wrong. All refs should be checked for accuracy and style consistancy. Many or all footnotes should be changed to endnotes (references).}

\end{enumerate}

\bibliography{text1}
\bibliographystyle{unsrt}

\end{document}