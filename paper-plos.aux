\relax 
\bibstyle{plos2009}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\citation{metzker,Marx:2013fk}
\citation{illumina}
\citation{bwa}
\citation{Langmead:2009uq}
\citation{samtools}
\citation{openmp}
\citation{top500}
\citation{Tange2011a}
\citation{mpi1}
\citation{pmap,erne,gnumap}
\citation{dean.2004.mapreduce}
\citation{lin2010}
\citation{hadoop,Hadoop:Guide}
\citation{Hadoop:Guide}
\citation{Sammer:2012}
\citation{gatk}
\citation{code_repo_bash}
\citation{code_repo_mr}
\citation{Langmead:2009kx}
\newlabel{sec:preprocessor}{{}{3}{MapReduce-based Data Preprocessor\relax }{section*.7}{}}
\citation{ncbi-sra}
\citation{lampa}
\citation{opennebula}
\citation{cloudera}
\citation{soapsnp}
\citation{schneeberger}
\citation{Langmead:2009kx,Pireddu:2011vn}
\citation{biostars_samtools}
\citation{hadoop,lin2010}
\citation{Sammer:2012}
\citation{pmap,gnumap}
\citation{Bozdag:2010cn}
\citation{code_repo_bash}
\citation{Langmead:2009uq}
\citation{lin2010,Hadoop:Guide}
\citation{galaxy}
\citation{cloudgene}
\newlabel{subsectionIV_2}{{}{7}{Usability aspects\relax }{section*.15}{}}
\bibdata{paper-plos}
\bibcite{metzker}{1}
\bibcite{Marx:2013fk}{2}
\bibcite{illumina}{3}
\bibcite{bwa}{4}
\bibcite{Langmead:2009uq}{5}
\bibcite{samtools}{6}
\bibcite{openmp}{7}
\bibcite{top500}{8}
\bibcite{Tange2011a}{9}
\bibcite{mpi1}{10}
\bibcite{pmap}{11}
\bibcite{erne}{12}
\bibcite{gnumap}{13}
\bibcite{dean.2004.mapreduce}{14}
\bibcite{lin2010}{15}
\bibcite{hadoop}{16}
\bibcite{Hadoop:Guide}{17}
\bibcite{Sammer:2012}{18}
\bibcite{gatk}{19}
\bibcite{code_repo_bash}{20}
\bibcite{code_repo_mr}{21}
\bibcite{Langmead:2009kx}{22}
\bibcite{ncbi-sra}{23}
\bibcite{lampa}{24}
\bibcite{opennebula}{25}
\bibcite{cloudera}{26}
\bibcite{soapsnp}{27}
\bibcite{schneeberger}{28}
\bibcite{Pireddu:2011vn}{29}
\bibcite{biostars_samtools}{30}
\bibcite{Bozdag:2010cn}{31}
\bibcite{galaxy}{32}
\bibcite{cloudgene}{33}
\bibcite{milouCluster}{34}
\bibcite{gulo}{35}
\citation{milouCluster}
\citation{gulo}
\citation{opennebula}
\citation{cloudera}
\@writefile{lot}{\contentsline {table}{\numberline {1}{\ignorespaces Time (in minutes) required by Samtools to sort Dataset IV (BAM file containing 58 Gbases). The measurements are done on a single HPC node with $p=16$ cores. Averaging is done over 5 independent simulations. The amount of available RAM reported is for all 16 cores. \relax }}{11}{table.caption.30}}
\newlabel{table:5}{{1}{11}{Time (in minutes) required by Samtools to sort Dataset IV (BAM file containing 58 Gbases). The measurements are done on a single HPC node with $p=16$ cores. Averaging is done over 5 independent simulations. The amount of available RAM reported is for all 16 cores. \relax \relax }{table.caption.30}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces Runtime for the Hadoop-based pipeline on the 56-core Hadoop cluster with increasing input dataset size. Each test dataset (labelled with Roman numerals) was used. The image clearly highlights the linear scaling characteristics of the Hadoop-based approach. \relax }}{12}{figure.caption.25}}
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:fig1}{{1}{12}{Runtime for the Hadoop-based pipeline on the 56-core Hadoop cluster with increasing input dataset size. Each test dataset (labelled with Roman numerals) was used. The image clearly highlights the linear scaling characteristics of the Hadoop-based approach. \relax \relax }{figure.caption.25}{}}
\@writefile{lot}{\contentsline {table}{\numberline {2}{\ignorespaces Time (in minutes) employed to preprocess Datasets II, III, and IV by Crossbow's native read preprocessor and our MR preprocessor on a 56-core Hadoop cluster, and by the custom multi-core Bash script on a single 16-core HPC node. \relax }}{12}{table.caption.31}}
\newlabel{table:preprocess}{{2}{12}{Time (in minutes) employed to preprocess Datasets II, III, and IV by Crossbow's native read preprocessor and our MR preprocessor on a 56-core Hadoop cluster, and by the custom multi-core Bash script on a single 16-core HPC node. \relax \relax }{table.caption.31}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces The ratio of the\nobreakspace  {}$F_{Hadoop}/F_{HPC}$ as a function of a reciprocal dataset size in Gbases. The pipelines were run on a 56-core Hadoop cluster and a 16-core HPC node, respectively. The points are fit to a quadratic least-squares curve, which makes it possible to predict {\it  infinite} dataset size. Datasets are labeled with Roman numerals. \relax }}{13}{figure.caption.26}}
\newlabel{fig:fig2}{{2}{13}{The ratio of the~$F_{Hadoop}/F_{HPC}$ as a function of a reciprocal dataset size in Gbases. The pipelines were run on a 56-core Hadoop cluster and a 16-core HPC node, respectively. The points are fit to a quadratic least-squares curve, which makes it possible to predict {\it infinite} dataset size. Datasets are labeled with Roman numerals. \relax \relax }{figure.caption.26}{}}
\@writefile{lot}{\contentsline {table}{\numberline {3}{\ignorespaces Timings for Dataset I processed on different number of cores of a HPC node.\relax }}{13}{table.caption.32}}
\newlabel{table:3}{{3}{13}{Timings for Dataset I processed on different number of cores of a HPC node.\relax \relax }{table.caption.32}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces An example of a job setup view with the graphical Hadoop front-end Cloudgene, providing a smooth user experience even for novice users. \textbf  {a)} Pipeline selection -- in our case containing the Crossbow pipeline. \textbf  {b)} The UPPMAX-adapted functionality to browsing and import data from the user's home folder in the shared file system. \relax }}{14}{figure.caption.27}}
\newlabel{fig:fig4}{{3}{14}{An example of a job setup view with the graphical Hadoop front-end Cloudgene, providing a smooth user experience even for novice users. \textbf {a)} Pipeline selection -- in our case containing the Crossbow pipeline. \textbf {b)} The UPPMAX-adapted functionality to browsing and import data from the user's home folder in the shared file system. \relax \relax }{figure.caption.27}{}}
\@writefile{lot}{\contentsline {table}{\numberline {4}{\ignorespaces Time (in minutes) for processing different dataset sizes on the HPC and Hadoop deployments with their respective pipelines. Datasets are labelled by Roman numerals, (``f.r.'' stands for ``forward reads''). The large variance for the HPC deployment is due to Samtools BAM sorting, which is I/O and memory intensive; the data spillage to the file system makes it very susceptible to the load of the entire shared HPC cluster. \relax }}{14}{table.caption.33}}
\newlabel{table:4}{{4}{14}{Time (in minutes) for processing different dataset sizes on the HPC and Hadoop deployments with their respective pipelines. Datasets are labelled by Roman numerals, (``f.r.'' stands for ``forward reads''). The large variance for the HPC deployment is due to Samtools BAM sorting, which is I/O and memory intensive; the data spillage to the file system makes it very susceptible to the load of the entire shared HPC cluster. \relax \relax }{table.caption.33}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces Ratios of alignment time\nobreakspace  {}$T_{alignment}$ to communication costs\nobreakspace  {}$T_{comm}$ for the HPC and Hadoop clusters as a function of reciprocal number of nodes\nobreakspace  {}$1/N$. All runs used Dataset\nobreakspace  {}IV as input. Two HPC scenarios are shown as ``HPC SLURM'' and ``HPC random'', which correspond to standard SLURM behavior and a modified one that allocates nodes from random racks. Linear fit to $ax+b$ was done with the least-squares method. The estimated coefficients for the Hadoop-based approach are $(a,b)\approx (0.95,2.07)$. One can see two defined linear regions for the HPC approach with very different tangents for linear and non-linear scalings correspondingly. For the ``HPC SLURM'' $(a,b)\approx (0.96, 3.49)$ and $(a,b)\approx (32.99, 0.60)$, and for the ``HPC random'' $(a,b)\approx (0.88, 3.72)$ and $(a,b)\approx (98.24, 0.53)$. The constant term for HPC goes below unity -- i.e.\ for a large number of nodes more time will be spent on communication than the actual alignment. \relax }}{15}{figure.caption.28}}
\newlabel{fig:fig3}{{4}{15}{Ratios of alignment time~$T_{alignment}$ to communication costs~$T_{comm}$ for the HPC and Hadoop clusters as a function of reciprocal number of nodes~$1/N$. All runs used Dataset~IV as input. Two HPC scenarios are shown as ``HPC SLURM'' and ``HPC random'', which correspond to standard SLURM behavior and a modified one that allocates nodes from random racks. Linear fit to $ax+b$ was done with the least-squares method. The estimated coefficients for the Hadoop-based approach are $(a,b)\approx (0.95,2.07)$. One can see two defined linear regions for the HPC approach with very different tangents for linear and non-linear scalings correspondingly. For the ``HPC SLURM'' $(a,b)\approx (0.96, 3.49)$ and $(a,b)\approx (32.99, 0.60)$, and for the ``HPC random'' $(a,b)\approx (0.88, 3.72)$ and $(a,b)\approx (98.24, 0.53)$. The constant term for HPC goes below unity -- i.e.\ for a large number of nodes more time will be spent on communication than the actual alignment. \relax \relax }{figure.caption.28}{}}
\@writefile{lot}{\contentsline {table}{\numberline {5}{\ignorespaces Datasets used in the comparison. \relax }}{15}{table.caption.34}}
\newlabel{table:datasets}{{5}{15}{Datasets used in the comparison. \relax \relax }{table.caption.34}{}}
\@writefile{lot}{\contentsline {table}{\numberline {6}{\ignorespaces Timings for alignment and the ratio\nobreakspace  {}$T_{alignment}/T_{comm}$ for HPC and Hadoop clusters for Dataset IV. For the ``HPC random'' approach, data chunks have to be copied to the local scratch disks first and the alignments (SAM files) copied back while Hadoop keeps all the data inside HDFS and hence does not need data staging. Hadoop however needs to preprocess reads before the actual alignment stage in order to be able to operate in MR manner resulting in what we term ``communication costs''. Note that each HPC node has 16 cores, while each Hadoop node has 7 (one core is dedicated to run the virtual machine).\relax }}{16}{table.caption.35}}
\newlabel{table:6}{{6}{16}{Timings for alignment and the ratio~$T_{alignment}/T_{comm}$ for HPC and Hadoop clusters for Dataset IV. For the ``HPC random'' approach, data chunks have to be copied to the local scratch disks first and the alignments (SAM files) copied back while Hadoop keeps all the data inside HDFS and hence does not need data staging. Hadoop however needs to preprocess reads before the actual alignment stage in order to be able to operate in MR manner resulting in what we term ``communication costs''. Note that each HPC node has 16 cores, while each Hadoop node has 7 (one core is dedicated to run the virtual machine).\relax \relax }{table.caption.35}{}}
