\relax 
\providecommand\hyper@newdestlabel[2]{}
\bibstyle{plos2009}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\citation{metzker,Marx:2013fk}
\citation{illumina}
\citation{bwa}
\citation{Langmead:2009uq}
\citation{samtools}
\citation{openmp}
\citation{top500}
\citation{Tange2011a}
\citation{mpi1}
\citation{pmap,erne,gnumap}
\citation{hadoop}
\citation{lin2010}
\citation{hadoop,Hadoop:Guide}
\citation{Hadoop:Guide}
\citation{Sammer:2012}
\citation{Langmead:2009kx}
\citation{ncbi-sra}
\citation{schneeberger}
\citation{biostars_samtools}
\citation{Langmead:2009kx,Pireddu:2011vn}
\citation{hadoop,lin2010}
\citation{Sammer:2012}
\citation{pmap,gnumap}
\citation{Bozdag:2010cn}
\citation{code_repo}
\citation{Langmead:2009uq}
\citation{lin2010,Hadoop:Guide}
\citation{galaxy}
\citation{cloudgene}
\newlabel{subsectionIV_2}{{}{5}{Usability aspects}{section*.10}{}}
\citation{gatk}
\citation{soapsnp}
\citation{code_repo}
\citation{code_repo}
\citation{lampa}
\citation{opennebula}
\citation{cloudera}
\bibdata{paper-plos}
\bibcite{metzker}{1}
\bibcite{Marx:2013fk}{2}
\bibcite{illumina}{3}
\bibcite{bwa}{4}
\bibcite{Langmead:2009uq}{5}
\bibcite{samtools}{6}
\bibcite{openmp}{7}
\bibcite{top500}{8}
\bibcite{Tange2011a}{9}
\bibcite{mpi1}{10}
\bibcite{pmap}{11}
\bibcite{erne}{12}
\bibcite{gnumap}{13}
\bibcite{hadoop}{14}
\bibcite{lin2010}{15}
\bibcite{Hadoop:Guide}{16}
\bibcite{Sammer:2012}{17}
\bibcite{Langmead:2009kx}{18}
\bibcite{ncbi-sra}{19}
\bibcite{schneeberger}{20}
\bibcite{biostars_samtools}{21}
\bibcite{Pireddu:2011vn}{22}
\bibcite{Bozdag:2010cn}{23}
\bibcite{code_repo}{24}
\bibcite{galaxy}{25}
\bibcite{cloudgene}{26}
\bibcite{gatk}{27}
\bibcite{soapsnp}{28}
\bibcite{lampa}{29}
\bibcite{opennebula}{30}
\bibcite{cloudera}{31}
\bibcite{milouCluster}{32}
\bibcite{gulo}{33}
\citation{milouCluster}
\citation{gulo}
\citation{opennebula}
\citation{cloudera}
\@writefile{lot}{\contentsline {table}{\numberline {1}{\ignorespaces Timings (in minutes) for the sort routine of the Samtools package for the dataset IV (58 Gbases BAM file). The measurements are done on a single HPC node with $p=16$ cores. Averaging is done over 5 independent simulations. The amount of RAM is given for all 16 cores. \relax }}{10}{table.caption.29}}
\newlabel{table:5}{{1}{10}{Timings (in minutes) for the sort routine of the Samtools package for the dataset IV (58 Gbases BAM file). The measurements are done on a single HPC node with $p=16$ cores. Averaging is done over 5 independent simulations. The amount of RAM is given for all 16 cores. \relax }{table.caption.29}{}}
\@writefile{lot}{\contentsline {table}{\numberline {2}{\ignorespaces Timings (in minutes) with Crossbow's native read preprocessor and with derived approach for datasets II, III, IV on a 56 cores Hadoop cluster, using the Crossbow, and with the BASH script on a single HPC node with 16 cores.\relax }}{10}{table.caption.30}}
\newlabel{table:preprocess}{{2}{10}{Timings (in minutes) with Crossbow's native read preprocessor and with derived approach for datasets II, III, IV on a 56 cores Hadoop cluster, using the Crossbow, and with the BASH script on a single HPC node with 16 cores.\relax }{table.caption.30}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces Calculation time for selected datasets (labeled with roman numerals) for the 56 cores Hadoop cluster shows a linear scaling with dataset size. \relax }}{11}{figure.caption.24}}
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:fig1}{{1}{11}{Calculation time for selected datasets (labeled with roman numerals) for the 56 cores Hadoop cluster shows a linear scaling with dataset size. \relax }{figure.caption.24}{}}
\@writefile{lot}{\contentsline {table}{\numberline {3}{\ignorespaces Timings for Dataset I processed on different number of cores of a HPC node.\relax }}{11}{table.caption.31}}
\newlabel{table:3}{{3}{11}{Timings for Dataset I processed on different number of cores of a HPC node.\relax }{table.caption.31}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces The ratio of the\nobreakspace  {}$F_{Hadoop}/F_{HPC}$ as a function of a reciprocal dataset size in Gbases. Calculations were carried out for 56 cores Hadoop and 16 cores HPC cluster correspondingly. The points are fit to a quadratic least-squares curve, which makes it possible to predict {\it  infinite} dataset size. Datasets are labeled with roman numerals.\relax }}{12}{figure.caption.25}}
\newlabel{fig:fig2}{{2}{12}{The ratio of the~$F_{Hadoop}/F_{HPC}$ as a function of a reciprocal dataset size in Gbases. Calculations were carried out for 56 cores Hadoop and 16 cores HPC cluster correspondingly. The points are fit to a quadratic least-squares curve, which makes it possible to predict {\it infinite} dataset size. Datasets are labeled with roman numerals.\relax }{figure.caption.25}{}}
\@writefile{lot}{\contentsline {table}{\numberline {4}{\ignorespaces Timings (in minutes) for HPC and Hadoop deployments for different dataset sizes. Dataset are labelled by roman numerals, ``f.r.'' stands for ``forward reads''. The large variance for the HPC deployment is due to Samtools BAM sorting, which is IO and memory intensive, thus strongly depend on the queuing at the HPC cluster.\relax }}{12}{table.caption.32}}
\newlabel{table:4}{{4}{12}{Timings (in minutes) for HPC and Hadoop deployments for different dataset sizes. Dataset are labelled by roman numerals, ``f.r.'' stands for ``forward reads''. The large variance for the HPC deployment is due to Samtools BAM sorting, which is IO and memory intensive, thus strongly depend on the queuing at the HPC cluster.\relax }{table.caption.32}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces An example of a job setup with the graphical Hadoop front-end Cloudgene, providing a smooth user experience even for novice users. \textbf  {a)} Pipeline selection - in our case containing the Crossbow pipeline. \textbf  {b)} The UPPMAX-adapted functionality to browsing and import data from the user's home folder in the shared file system.\relax }}{13}{figure.caption.26}}
\newlabel{fig:fig4}{{3}{13}{An example of a job setup with the graphical Hadoop front-end Cloudgene, providing a smooth user experience even for novice users. \textbf {a)} Pipeline selection - in our case containing the Crossbow pipeline. \textbf {b)} The UPPMAX-adapted functionality to browsing and import data from the user's home folder in the shared file system.\relax }{figure.caption.26}{}}
\@writefile{lot}{\contentsline {table}{\numberline {5}{\ignorespaces Datasets used in the comparison. \relax }}{13}{table.caption.33}}
\newlabel{table:datasets}{{5}{13}{Datasets used in the comparison. \relax }{table.caption.33}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces Ratios of the alignment time\nobreakspace  {}$T_{alignment}$ to the communication costs\nobreakspace  {}$T_{comm}$ for HPC and Hadoop clusters for Dataset\nobreakspace  {}IV as a function of reciprocal number of nodes\nobreakspace  {}$1/N$. Two HPC scenarios are shown as ``HPC SLURM'' and ``HPC random'', which correspond to standard SLURM behavior and a modified one where nodes are being allocated from random racks. Linear fit to $ax+b$ was done with the least-squares method. The estimated values for Hadoop approach are $(a,b)\approx (0.95,2.07)$. One can see two defined linear regions for the HPC approach with very different tangents for linear and non-linear scalings correspondingly. For the ``HPC SLURM'' $(a,b)\approx (0.96, 3.49)$ and $(a,b)\approx (32.99, 0.60)$, and for the ``HPC random'' $(a,b)\approx (0.88, 3.72)$ and $(a,b)\approx (98.24, 0.53)$. The constant term for HPC goes below unity, i.e. for the large number of nodes the communication will take more time than actual alignment. \relax }}{14}{figure.caption.27}}
\newlabel{fig:fig3}{{4}{14}{Ratios of the alignment time~$T_{alignment}$ to the communication costs~$T_{comm}$ for HPC and Hadoop clusters for Dataset~IV as a function of reciprocal number of nodes~$1/N$. Two HPC scenarios are shown as ``HPC SLURM'' and ``HPC random'', which correspond to standard SLURM behavior and a modified one where nodes are being allocated from random racks. Linear fit to $ax+b$ was done with the least-squares method. The estimated values for Hadoop approach are $(a,b)\approx (0.95,2.07)$. One can see two defined linear regions for the HPC approach with very different tangents for linear and non-linear scalings correspondingly. For the ``HPC SLURM'' $(a,b)\approx (0.96, 3.49)$ and $(a,b)\approx (32.99, 0.60)$, and for the ``HPC random'' $(a,b)\approx (0.88, 3.72)$ and $(a,b)\approx (98.24, 0.53)$. The constant term for HPC goes below unity, i.e. for the large number of nodes the communication will take more time than actual alignment. \relax }{figure.caption.27}{}}
\@writefile{lot}{\contentsline {table}{\numberline {6}{\ignorespaces Timings for alignment and the ratio\nobreakspace  {}$T_{alignment}/T_{comm}$ for HPC and Hadoop clusters for Dataset IV. For the ``HPC random'' approach, data chunks have to be copied to the local scratch disks first and the alignments (SAM files) copied back while Hadoop keeps all the data inside HDFS and hence does not need data staging. Hadoop however needs to preprocess reads before the actual alignment stage in order to be able to operate in MR manner resulting in what we term ``communication costs''. Note that each HPC node has 16 cores, while each Hadoop node has 7 (one core is dedicated to run the virtual machine).\relax }}{15}{table.caption.34}}
\newlabel{table:6}{{6}{15}{Timings for alignment and the ratio~$T_{alignment}/T_{comm}$ for HPC and Hadoop clusters for Dataset IV. For the ``HPC random'' approach, data chunks have to be copied to the local scratch disks first and the alignments (SAM files) copied back while Hadoop keeps all the data inside HDFS and hence does not need data staging. Hadoop however needs to preprocess reads before the actual alignment stage in order to be able to operate in MR manner resulting in what we term ``communication costs''. Note that each HPC node has 16 cores, while each Hadoop node has 7 (one core is dedicated to run the virtual machine).\relax }{table.caption.34}{}}
