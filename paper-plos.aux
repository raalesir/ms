\relax 
\providecommand\hyper@newdestlabel[2]{}
\bibstyle{plos2009}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\citation{metzker,Marx:2013fk}
\citation{illumina}
\citation{bwa}
\citation{Langmead:2009uq}
\citation{samtools}
\citation{openmp}
\citation{gnuparallel}
\citation{mpi1}
\citation{pmap,erne,gnumap}
\citation{hadoop}
\citation{lin2010}
\citation{hadoop,Hadoop:Guide}
\citation{Hadoop:Guide}
\citation{Sammer:2012}
\citation{Langmead:2009kx}
\citation{ncbi-sra}
\citation{schneeberger}
\citation{Langmead:2009kx,Pireddu:2011vn}
\citation{hadoop,lin2010}
\citation{Sammer:2012}
\@writefile{lot}{\contentsline {table}{\numberline {1}{\ignorespaces Timings (in minutes) for the sort routine of the Samtools package for the dataset IV (58 Gb BAM file). The measurements are done on a single HPC node with $p=16$ cores. Averaging is done over 5 independent simulations. The amount of RAM is given for all 16 cores. \relax }}{4}{table.caption.9}}
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{table:5}{{1}{4}{Timings (in minutes) for the sort routine of the Samtools package for the dataset IV (58 Gb BAM file). The measurements are done on a single HPC node with $p=16$ cores. Averaging is done over 5 independent simulations. The amount of RAM is given for all 16 cores. \relax }{table.caption.9}{}}
\citation{pmap,gnumap}
\citation{Bozdag:2010cn}
\citation{code_repo}
\citation{Langmead:2009uq}
\citation{galaxy}
\citation{cloudgene}
\newlabel{subsectionIV_2}{{}{6}{Usability aspects}{section*.11}{}}
\citation{gatk}
\citation{soapsnp}
\citation{code_repo}
\citation{code_repo}
\citation{lampa}
\citation{opennebula}
\citation{cloudera}
\bibdata{paper-plos}
\bibcite{metzker}{1}
\bibcite{Marx:2013fk}{2}
\bibcite{illumina}{3}
\bibcite{bwa}{4}
\bibcite{Langmead:2009uq}{5}
\bibcite{samtools}{6}
\bibcite{openmp}{7}
\bibcite{gnuparallel}{8}
\bibcite{mpi1}{9}
\bibcite{pmap}{10}
\bibcite{erne}{11}
\bibcite{gnumap}{12}
\bibcite{hadoop}{13}
\bibcite{lin2010}{14}
\bibcite{Hadoop:Guide}{15}
\bibcite{Sammer:2012}{16}
\bibcite{Langmead:2009kx}{17}
\bibcite{ncbi-sra}{18}
\bibcite{schneeberger}{19}
\bibcite{Pireddu:2011vn}{20}
\bibcite{Bozdag:2010cn}{21}
\bibcite{code_repo}{22}
\bibcite{galaxy}{23}
\bibcite{cloudgene}{24}
\bibcite{gatk}{25}
\bibcite{soapsnp}{26}
\bibcite{lampa}{27}
\bibcite{opennebula}{28}
\bibcite{cloudera}{29}
\bibcite{milouCluster}{30}
\bibcite{gulo}{31}
\citation{milouCluster}
\citation{gulo}
\citation{opennebula}
\citation{cloudera}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces Calculation time for selected datasets (labeled with roman numerals) for the 56 cores Hadoop cluster shows a linear scaling with dataset size. \relax }}{11}{figure.caption.25}}
\newlabel{fig:fig1}{{1}{11}{Calculation time for selected datasets (labeled with roman numerals) for the 56 cores Hadoop cluster shows a linear scaling with dataset size. \relax }{figure.caption.25}{}}
\@writefile{lot}{\contentsline {table}{\numberline {2}{\ignorespaces Timings (in minutes) with Crossbow's native read preprocessor and with derived approach for datasets II, III, IV on a 56 cores Hadoop cluster, using the Crossbow, and with the BASH script on a single HPC node with 16 cores.\relax }}{11}{table.caption.30}}
\newlabel{table:preprocess}{{2}{11}{Timings (in minutes) with Crossbow's native read preprocessor and with derived approach for datasets II, III, IV on a 56 cores Hadoop cluster, using the Crossbow, and with the BASH script on a single HPC node with 16 cores.\relax }{table.caption.30}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces The ratio of the\nobreakspace  {}$F_{Hadoop}/F_{HPC}$ as a function of a reciprocal dataset size in Gigabases. Calculations were carried out for 56 cores Hadoop and 16 cores HPC cluster correspondingly. The points are fit to a quadratic least-squares curve, which makes it possible to predict {\it  infinite} dataset size. Datasets are labeled with roman numerals.\relax }}{12}{figure.caption.26}}
\newlabel{fig:fig2}{{2}{12}{The ratio of the~$F_{Hadoop}/F_{HPC}$ as a function of a reciprocal dataset size in Gigabases. Calculations were carried out for 56 cores Hadoop and 16 cores HPC cluster correspondingly. The points are fit to a quadratic least-squares curve, which makes it possible to predict {\it infinite} dataset size. Datasets are labeled with roman numerals.\relax }{figure.caption.26}{}}
\@writefile{lot}{\contentsline {table}{\numberline {3}{\ignorespaces Calculation time for Dataset I executed on different number of cores on a node of HPC cluster. {\color  {red} awkward title, consider replacing with graph (and new title) }\relax }}{12}{table.caption.31}}
\newlabel{table:3}{{3}{12}{Calculation time for Dataset I executed on different number of cores on a node of HPC cluster. \COMMENT {awkward title, consider replacing with graph (and new title)}\relax }{table.caption.31}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces An example of a job setup with the graphical Hadoop front-end Cloudgene, providing a smooth user experience even for novice users. \textbf  {a)} Pipeline selection - in our case containing the Crossbow pipeline. \textbf  {b)} The UPPMAX-adapted functionality to browsing and import data from the user's home folder in the shared file system.\relax }}{13}{figure.caption.27}}
\newlabel{fig:fig4}{{3}{13}{An example of a job setup with the graphical Hadoop front-end Cloudgene, providing a smooth user experience even for novice users. \textbf {a)} Pipeline selection - in our case containing the Crossbow pipeline. \textbf {b)} The UPPMAX-adapted functionality to browsing and import data from the user's home folder in the shared file system.\relax }{figure.caption.27}{}}
\@writefile{lot}{\contentsline {table}{\numberline {4}{\ignorespaces Timings (in minutes) for HPC and Hadoop deployments for different dataset sizes. Dataset is shown in brackets by roman numerals, ``f.r.'' stands for ``forward reads''. Big deviation {\color  {red} what aspect of deviation, not fixable? } for the HPC is due to Samtools BAM sorting, which is IO and memory intensive, thus strongly depend on the queuing at the HPC cluster. {\color  {red} No p= used here. } \relax }}{13}{table.caption.32}}
\newlabel{table:4}{{4}{13}{Timings (in minutes) for HPC and Hadoop deployments for different dataset sizes. Dataset is shown in brackets by roman numerals, ``f.r.'' stands for ``forward reads''. Big deviation \COMMENT {what aspect of deviation, not fixable?} for the HPC is due to Samtools BAM sorting, which is IO and memory intensive, thus strongly depend on the queuing at the HPC cluster. \COMMENT {No p= used here.} \relax }{table.caption.32}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces Ratios of the mapping time\nobreakspace  {}$T_{mapping}$ to the communication costs\nobreakspace  {}$T_{comm}$ for HPC and Hadoop clusters for Dataset IV as a function of reciprocal cluster size $1/N$. Two HPC scenarios are shown as ``HPC SLURM'' and ``HPC random'', which correspond to standard SLURM behaviour and a modified one where nodes are being allocated from random racks. Linear fit done with the least-squares method. {\color  {red} Figure legend too complex. Y-axis label inconsistant with previous figure. }\relax }}{14}{figure.caption.28}}
\newlabel{fig:fig3}{{4}{14}{Ratios of the mapping time~$T_{mapping}$ to the communication costs~$T_{comm}$ for HPC and Hadoop clusters for Dataset IV as a function of reciprocal cluster size $1/N$. Two HPC scenarios are shown as ``HPC SLURM'' and ``HPC random'', which correspond to standard SLURM behaviour and a modified one where nodes are being allocated from random racks. Linear fit done with the least-squares method. \COMMENT {Figure legend too complex. Y-axis label inconsistant with previous figure.}\relax }{figure.caption.28}{}}
\@writefile{lot}{\contentsline {table}{\numberline {5}{\ignorespaces Datasets used in the comparison. \relax }}{14}{table.caption.33}}
\newlabel{table:datasets}{{5}{14}{Datasets used in the comparison. \relax }{table.caption.33}{}}
\@writefile{lot}{\contentsline {table}{\numberline {6}{\ignorespaces Timings for mapping and the ratio\nobreakspace  {}$T_{mapping}/T_{comm}$ for HPC and Hadoop clusters for Dataset IV. For the ``HPC random'' approach, data chunks have to be copied to the local scratch disks first and the alignments (SAM files) copied back while Hadoop keeps all the data inside HDFS and hence does not need data staging. Hadoop however needs to preprocess reads before the actual alignment stage in order to be able to operate in MR manner resulting in what we term ``communication costs''. Note that each HPC node has 16 cores, while each Hadoop node has 7 (one core is dedicated to run the virtual machine).\relax }}{15}{table.caption.34}}
\newlabel{table:6}{{6}{15}{Timings for mapping and the ratio~$T_{mapping}/T_{comm}$ for HPC and Hadoop clusters for Dataset IV. For the ``HPC random'' approach, data chunks have to be copied to the local scratch disks first and the alignments (SAM files) copied back while Hadoop keeps all the data inside HDFS and hence does not need data staging. Hadoop however needs to preprocess reads before the actual alignment stage in order to be able to operate in MR manner resulting in what we term ``communication costs''. Note that each HPC node has 16 cores, while each Hadoop node has 7 (one core is dedicated to run the virtual machine).\relax }{table.caption.34}{}}
